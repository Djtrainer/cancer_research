{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Identifying Cellular Neighborhoods with Graph Neural Networks in Spatial Transcriptomics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Introduction\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### References\n",
    "\n",
    "**Data Sources and Platforms**\n",
    "* [10X Genomics](https://www.10xgenomics.com/datasets/human-breast-cancer-block-a-section-1-1-standard-1-1-0): Invasive Ductal Carcinoma tissue.\n",
    "\n",
    "**Libraries**\n",
    "* [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "\n",
    "* [SquidPy Docs](https://squidpy.readthedocs.io/en/stable/)\n",
    "\n",
    "* [ScanPy Docs](https://scanpy.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Imports](#imports)\n",
    "2. [Constants & Data](#constants)\n",
    "3. [...](#distributions)\n",
    "\n",
    "6. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling, train_test_split_edges\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Constants & Data Import <a class=\"anchor\" id=\"constants\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The necessary data files can be accessed and downloaded via the [10X Genomics Datasets Site](https://www.10xgenomics.com/datasets/human-breast-cancer-block-a-section-1-1-standard-1-1-0). The following specific datasets are required:\n",
    "\n",
    "* **Filtered feature barcode matrix**\n",
    "\n",
    "    *V1_Breast_Cancer_Block_A_Section_1_filtered_feature_bc_matrix* \n",
    "    \n",
    "    This is your core gene expression data that contains a matrix where the rows are genes and the columns are the unique barcodes for each spot on the tissue slide.\n",
    "\n",
    "* **Spatial imaging data**\n",
    "\n",
    "    *V1_Breast_Cancer_Block_A_Section_1_spatial.tar.gz*\n",
    "\n",
    "    Compressed folder contains all the crucial spatial information including files like tissue_positions.csv, which has the exact (x, y) pixel coordinates for every spot/barcode. \n",
    "\n",
    "* **Tissue Image**\n",
    "\n",
    "    *V1_Breast_Cancer_Block_A_Section_1_image.tif*\n",
    "\n",
    "    High-resolution H&E stained image of the tissue slice\n",
    "\n",
    "\n",
    "The notebook expects both tsv files to be placed in the `data/10x` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(os.getcwd()).parents[0]\n",
    "\n",
    "DATA_PATH = os.path.join(ROOT, \"data\", \"10x\")\n",
    "SPATIAL_DATA_PATH = os.path.join(DATA_PATH, \"spatial\")\n",
    "H5_DATA_PATH = os.path.join(\n",
    "    DATA_PATH, \"V1_Breast_Cancer_Block_A_Section_1_filtered_feature_bc_matrix.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Data Exploration <a class=\"anchor\" id=\"constants\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into an AnnData object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_visium(path=DATA_PATH, count_file=H5_DATA_PATH)\n",
    "adata.var_names_make_unique()  # Ensure gene names are unique\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(\n",
    "    adata, min_cells=10\n",
    ")  # Filter out genes expressed in fewer than 10 spots\n",
    "sc.pp.normalize_total(adata, inplace=True)  # Normalize counts per spot\n",
    "sc.pp.log1p(adata)  # Log-transform the data\n",
    "# Calculate quality control metrics, adding total_counts (total number of gene counts per spot)\n",
    "sc.pp.calculate_qc_metrics(adata, inplace=True)\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adata.uns, adata.obsm**\n",
    "\n",
    "Display the image of the slide from`adata.uns` and plot the scatter plot from the spatial spot coordinates from `adata.obsm` (colored by the total counts in `adata.obs` to indicate expression per spot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata.uns[\"spatial\"][\"V1_Breast_Cancer_Block_A_Section_1\"].keys())\n",
    "print(print(adata.uns[\"spatial\"][\"V1_Breast_Cancer_Block_A_Section_1\"][\"metadata\"]))\n",
    "_, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(\n",
    "    adata.uns[\"spatial\"][\"V1_Breast_Cancer_Block_A_Section_1\"][\"images\"][\"hires\"]\n",
    ")\n",
    "axes[0].set_title(\"High Resolution Image\")\n",
    "\n",
    "axes[1].set_title(\"Low Resolution Image\")\n",
    "axes[1].imshow(\n",
    "    adata.uns[\"spatial\"][\"V1_Breast_Cancer_Block_A_Section_1\"][\"images\"][\"lowres\"]\n",
    ")\n",
    "\n",
    "axes[2].scatter(\n",
    "    x=adata.obsm[\"spatial\"][:, 0],\n",
    "    y=adata.obsm[\"spatial\"][:, 1] * -1\n",
    "    - min(adata.obsm[\"spatial\"][:, 1] * -1),  # invert y-axis\n",
    "    c=adata.obs[\"total_counts\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=8,\n",
    ")\n",
    "axes[2].set_title(\"Spatial Plot of Total Counts\")\n",
    "axes[0].set_aspect(\"equal\")\n",
    "axes[1].set_aspect(\"equal\")\n",
    "axes[2].set_aspect(\"equal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adata.var**\n",
    "\n",
    "Each row in adata.var corresponds to a single gene. The important columns in adata.var for this notebook include:\n",
    "\n",
    "* `gene_ids`: The Ensembl ID for the gene.\n",
    "* `feature_types`: The type of featureb entirely \"Gene Expression\" for this data.\n",
    "* `genome`: The reference genome used.\n",
    "* `n_cells`: A column added by scanpy after filtering, showing in how many spots (cells) each gene was detected.\n",
    "* `total_counts`: A quality control metric showing the total number of counts for each gene across all spots.\n",
    "* `mean_counts`: average expression across the data set\n",
    "* `log1p_mean_counts`: log-transformed version of `mean_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata.var.shape)\n",
    "adata.var.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adata.var**\n",
    "\n",
    "Each row in adata.obs corresponds to a observation, or spot on the tissue slide. The important columns in adata.obs for this notebook include:\n",
    "\n",
    "* `in_tissue`: A binary column (0 or 1) indicating whether the spot is located over the actual tissue section (1) or in the background (0).\n",
    "* `array_row` and `array_col`: These are the integer row and column coordinates of the spot on the physical grid of the Visium slide.\n",
    "* `total_counts`: This is the total number of gene transcripts (UMIs) detected in that specific spot. It's a measure of the sequencing depth or \"library size\" for that spot. \n",
    "* `n_genes_by_counts`: The number of unique genes that were detected in that spot. A very low number might indicate a low-quality spot.\n",
    "* `log1p_total_counts`: The log-transformed version of total_counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata.obs.shape)\n",
    "adata.obs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a graph using squidpy's 'spatial_neighbors' function which builds a graph by connecting each spot to its nearest neighbors.  The hue on the scatter plot corresponds to the total number of gene transcripts detected in each spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.gr.spatial_neighbors(adata, coord_type=\"grid\", n_neighs=6)\n",
    "for k, v in adata.obsp.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "_, axes = plt.subplots(figsize=(6, 6))\n",
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    library_id=\"spatial\",  # Use the key for the image\n",
    "    color=\"total_counts\",  # Color spots by total gene counts as an example\n",
    "    shape=None,\n",
    "    connectivity_key=\"spatial_connectivities\",  # Tell squidpy to draw the graph\n",
    "    title=\"Spatial Graph Overlay on Tissue\",\n",
    "    ax=axes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "* This slide contains 3,798 spots and 19,690 genes sampled.\n",
    "* Total counts seem to correlate with cell density. The areas with the highest total_counts (the bright yellow spots) visually align with the densest, most purple regions in the H&E image. Conversely, the blue/darker areas with lower counts correspond to less dense regions (like stroma or connective tissue).\n",
    "* The clear spatial pattern of high-count and low-count regions demonstrates that the tissue is heterogeneous. This heterogeneity should be leveraged by our GNN to learn if and where there are different types of genes being expressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Data Pre-processing <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critical data for the GNN will be \n",
    "* `adata.X`: a sparse matrix that is n (number of spots) x m (number of genes) in size that contains gene expression at each spot.\n",
    "* `adata.obs['spatial_connectivities']`: the graph adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the gene expression matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_highest_gene_idx = np.argsort(adata.X.toarray().mean(axis=0))[::-1][:20]\n",
    "bottom_ten_highest_gene_idx = np.argsort(adata.X.toarray().mean(axis=0))[::-1][-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_sample_of_spots = np.random.randint(0, adata.X.toarray().shape[0], 40)\n",
    "\n",
    "# sample_array = np.concat([\n",
    "#     adata.X.toarray()[random_sample_of_spots, :][:, top_ten_highest_gene_idx],\n",
    "#     adata.X.toarray()[random_sample_of_spots, :][:, bottom_ten_highest_gene_idx]\n",
    "# ], axis=1)\n",
    "\n",
    "# _, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# sns.heatmap(\n",
    "#     adata.X.toarray(),\n",
    "#     cmap=\"RdBu_r\",\n",
    "#     cbar_kws={\"label\": \"Expression Level\"},\n",
    "#     xticklabels=[],\n",
    "#     yticklabels=[],\n",
    "#     cbar=False,\n",
    "#     square = False,\n",
    "#     ax=axes[0]\n",
    "#     )\n",
    "\n",
    "# sns.heatmap(\n",
    "#     sample_array,\n",
    "#     cmap=\"RdBu_r\",\n",
    "#     cbar_kws={\"label\": \"Expression Level\"},\n",
    "#     xticklabels=np.concatenate([\n",
    "#         adata.var_names[top_ten_highest_gene_idx],\n",
    "#         adata.var_names[bottom_ten_highest_gene_idx]\n",
    "#         ]),\n",
    "#     yticklabels=[],\n",
    "#     cbar=False,\n",
    "#     square = True,\n",
    "#     ax=axes[1]\n",
    "#     )\n",
    "\n",
    "# axes[0].set_title(\"Gene Expression Matrix\")\n",
    "# axes[0].set_xlabel(\"Genes\")\n",
    "# axes[0].set_ylabel(\"Spots\")\n",
    "\n",
    "# axes[1].set_title(\"Highest and Lowest Expressed Genes\")\n",
    "# axes[1].set_xlabel(\"Genes\")\n",
    "# axes[1].set_ylabel(f\"{len(random_sample_of_spots)} Random Spots\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some genes have relatively high expression over all spots in the slide indicated by the vertical lines in the heatmap above while others aren't expressed at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the spatial connectivites from it's native matrix to COOrdinate sparse matrix. This format has three key components:\n",
    "\n",
    "1. .row: A list of row indices for every non-zero element.\n",
    "2. .col: A list of column indices for every non-zero element.\n",
    "3. .data: A list of the actual values at those (row, col) positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsp[\"spatial_connectivities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsp[\"spatial_connectivities\"].tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_coo = adata.obsp[\"spatial_connectivities\"].tocoo()\n",
    "edge_index = torch.tensor(\n",
    "    np.vstack([edge_index_coo.row, edge_index_coo.col]), dtype=torch.long\n",
    ")\n",
    "\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this edge_index coordinate representation, we see each node connected to at most 6 other nodes and both directions counted (i.e., node_a ==> node_b and node_b ==> node_a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_per_node = pd.DataFrame(edge_index.flatten()).value_counts().values\n",
    "\n",
    "edge_per_node_counts, edge_per_node_values = np.histogram(\n",
    "    edges_per_node, bins=np.linspace(0, 15, 16)\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=np.array([edge_per_node_values[:-1], edge_per_node_counts]).T,\n",
    "    columns=[\"Number of Edges\", \"Counts\"],\n",
    ").sort_values(\"Counts\", ascending=False).iloc[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Geometric models require a specific 'Data' object that holds the graph structure. Convert the Snapy AnnData to a compatible structure - COO (adjacency matrix) format for edges and dense torch tensor for the nodes (gene expression matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges - we will use the spatial connectivities computed by Squidpy\n",
    "edge_index_coo = adata.obsp[\"spatial_connectivities\"].tocoo()\n",
    "edge_index = torch.tensor(\n",
    "    np.vstack([edge_index_coo.row, edge_index_coo.col]), dtype=torch.long\n",
    ")\n",
    "\n",
    "# Nodes - we will use the gene expression matrix as node features\n",
    "x = torch.tensor(adata.X.toarray(), dtype=torch.float)\n",
    "\n",
    "# Create the PyG Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(\"--- PyTorch Geometric Data Object ---\")\n",
    "print(data)\n",
    "\n",
    "# Split edges into training, validation, and test sets\n",
    "data = train_test_split_edges(data, val_ratio=0.1, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Build the Model <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will build a Graph Autoencoder (GAE). This model has two parts:\n",
    "\n",
    "  - An Encoder: This is our GCN. Its job is to compress the high-dimensional\n",
    "    gene expression of each node into a low-dimensional embedding.\n",
    "\n",
    "  - A Decoder: It tries to reconstruct the original graph structure (the edges)\n",
    "    from the learned embeddings.\n",
    "\n",
    "The architecture of this model is based on the paper [Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308) as we're aiming to build a model that learn meaningful latent embeddings representing our gene expressions based on their relative locations within the sample.\n",
    "\n",
    "The encoder is comprised of Graph Convolutioanl Networks from [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907). ([blog](https://tkipf.github.io/graph-convolutional-networks/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        # First GCN layer: maps input features to an intermediate dimension\n",
    "        self.conv1 = GCNConv(in_channels, out_channels * 2)\n",
    "        # Second GCN layer: maps intermediate dimension to the final embedding dimension\n",
    "        self.conv2 = GCNConv(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply GCN layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # The final output is the node embedding\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GAEModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAEModel, self).__init__()\n",
    "        # The Encoder is our GCN model defined above\n",
    "        self.encoder = GCNEncoder(in_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        # Pass data through the encoder to get latent embeddings (z)\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        # For a given set of positive and negative edges, predict the likelihood of existence\n",
    "        # using an inner product decoder.\n",
    "        pos_logits = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=1)\n",
    "        neg_logits = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=1)\n",
    "        return pos_logits, neg_logits\n",
    "\n",
    "    def recon_loss(self, z, pos_edge_index):\n",
    "        # Sample negative edges (pairs of nodes that are not connected)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=pos_edge_index,\n",
    "            num_nodes=z.size(0),\n",
    "            num_neg_samples=pos_edge_index.size(1),  # Match number of positive edges\n",
    "        )\n",
    "\n",
    "        # Get predictions (logits) for positive and negative edges\n",
    "        pos_logits, neg_logits = self.decode(z, pos_edge_index, neg_edge_index)\n",
    "\n",
    "        # Create labels: 1s for positive edges, 0s for negative edges\n",
    "        pos_labels = torch.ones_like(pos_logits)\n",
    "        neg_labels = torch.zeros_like(neg_logits)\n",
    "\n",
    "        # Concatenate and compute binary cross-entropy loss\n",
    "        logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
    "        labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = data.num_features  # Number of genes\n",
    "out_channels = 32  # Desired size of the embedding for each spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAEModel(in_channels, out_channels)\n",
    "\n",
    "# move model, data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Train the model for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        float: The reconstruction loss on the training edges.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.train_pos_edge_index)\n",
    "    # calculate the loss on the training edges\n",
    "    loss = model.recon_loss(z, data.train_pos_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(pos_edge_index):\n",
    "    \"\"\"Evaluate the model on the validation edges.\n",
    "    \n",
    "    Args:\n",
    "        pos_edge_index (torch.Tensor): The positive edge indices for validation.\n",
    "    Returns:\n",
    "        float: The reconstruction loss on the validation edges.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.train_pos_edge_index)\n",
    "    # calculate the loss on the validation edges\n",
    "    loss = model.recon_loss(z, pos_edge_index)\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "# SAVE_INTERVAL = 1\n",
    "\n",
    "pbar = tqdm(range(1, NUM_EPOCHS + 1))\n",
    "saved_embeddings = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in pbar:\n",
    "\n",
    "    # if epoch % SAVE_INTERVAL == 0 or epoch == 1:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run inference on the full graph to get embeddings\n",
    "        full_z = model.encode(data.x, edge_index.to(device)).cpu().numpy()\n",
    "        saved_embeddings.append({'epoch': epoch, 'embeddings': full_z})\n",
    "            \n",
    "    train_loss = train()\n",
    "    val_loss = test(data.val_pos_edge_index)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Update the progress bar's description with the current loss\n",
    "    pbar.set_description(\n",
    "        f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# This function will be called for each frame of the animation\n",
    "def update(frame):\n",
    "    # Clear the previous plots\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "\n",
    "    # Get the data for the current frame\n",
    "    epoch_data = saved_embeddings[frame]\n",
    "    epoch = epoch_data['epoch']\n",
    "    embeddings = epoch_data['embeddings']\n",
    "    \n",
    "    # --- Perform clustering for the current frame ---\n",
    "    adata.obsm['GCN_temp_embeddings'] = embeddings\n",
    "    sc.pp.neighbors(adata, use_rep=\"GCN_temp_embeddings\")\n",
    "    sc.tl.leiden(adata, resolution=0.5, key_added=\"GCN_temp_clusters\", random_state=42)\n",
    "    \n",
    "    # --- Plot 1: Spatial Clustering ---\n",
    "    sq.pl.spatial_scatter(\n",
    "        adata,\n",
    "        color=\"GCN_temp_clusters\",\n",
    "        shape=None,\n",
    "        ax=ax1,\n",
    "        title=None,\n",
    "        legend_loc=None\n",
    "    )\n",
    "    ax1.set_title(f\"Spatial Neighborhoods\\nEpoch: {epoch}\")\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    # --- Plot 2: UMAP of Embeddings ---\n",
    "    sc.tl.umap(adata, min_dist=0.5, random_state=42)\n",
    "    sc.pl.umap(\n",
    "        adata,\n",
    "        color=\"GCN_temp_clusters\",\n",
    "        ax=ax2,\n",
    "        show=False,\n",
    "        title=None,\n",
    "        legend_loc='on data'\n",
    "    )\n",
    "    ax2.set_title(f\"UMAP of Embeddings\\nEpoch: {epoch}\")\n",
    "\n",
    "    # Set a single title for the entire figure for that frame\n",
    "    fig.suptitle(f\"GCN Training Convergence at Epoch {epoch}\", fontsize=16)\n",
    "\n",
    "\n",
    "# Create the animation object\n",
    "# The interval is the delay between frames in milliseconds\n",
    "ani = FuncAnimation(fig, update, frames=len(saved_embeddings), interval=500, repeat=False)\n",
    "\n",
    "# Display the animation in the notebook\n",
    "# This may take a minute or two to render\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.encode(data.x, edge_index.to(device))\n",
    "    final_embeddings = output.cpu().numpy()\n",
    "    \n",
    "# Add the learned embeddings back to our original AnnData object for easy use\n",
    "adata.obsm[\"GCN_embeddings\"] = final_embeddings\n",
    "\n",
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- In simple terms, this GNN learns a spatially-aware latent representation of the gene expression. Its GCN layers are built on the principle that local neighborhoods matter, and its contrastive loss function trains it to make the embeddings of neighboring nodes similar while making the embeddings of non-neighboring nodes dissimilar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Clustering & Visualization <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform Clustering on the GCN Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the nearest-neighbor graph in the embedding space using the learned embeddings stored in `adata.obsm['GCN_embeddings']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep=\"GCN_embeddings\", n_neighbors=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Leiden clustering algorithm.  The Leiden algorithm is a powerful community detection algorithm used to find groups of nodes that are more densely connected to each other than to the rest of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata, resolution=0.5, key_added=\"GCN_leiden_clusters\")\n",
    "adata.obs['GCN_leiden_clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the Cellular Neighborhoods\n",
    "\n",
    "Use squidpy's plotting function to color each spot on the tissue image according to its assigned cluster ID. This reveals the spatial organization of the cellular neighborhoods identified by the GNN. In the same pannel, use UMAP to visualize how the embeddings cluster in 2D space colored by their Leiden assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the naive model to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(\n",
    "    x=adata.obsm[\"spatial\"][:, 0],\n",
    "    y=adata.obsm[\"spatial\"][:, 1] * -1\n",
    "    - min(adata.obsm[\"spatial\"][:, 1] * -1),  # invert y-axis\n",
    "    c=adata.obs[\"total_counts\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=8,\n",
    ")\n",
    "axes[0].set_title(\"Spatial Plot of Total Counts\")\n",
    "axes[0].set_aspect(\"equal\")\n",
    "axes[1].set_aspect(\"equal\")\n",
    "axes[2].set_aspect(\"equal\")\n",
    "\n",
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    library_id=\"spatial\",\n",
    "    color=\"GCN_leiden_clusters\",\n",
    "    shape=None,\n",
    "    title=\"Cellular Neighborhoods Identified by GCN Clustering\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].legend([])\n",
    "sc.tl.umap(adata, min_dist=0.5)\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=\"GCN_leiden_clusters\",\n",
    "    title=\"UMAP of GCN Embeddings\",\n",
    "    ax=axes[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the genes that are most significantly up-regulated in each cluster using a t-test to find differentially expressed genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(\n",
    "    adata, \n",
    "    groupby='GCN_leiden_clusters', \n",
    "    method='t-test', \n",
    "    key_added='marker_genes'\n",
    ")\n",
    "\n",
    "# Visualize top marker genes for each cluster\n",
    "pd.DataFrame(adata.uns['marker_genes']['names']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dot plot of the top 4 marker genes for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.rank_genes_groups_dotplot(\n",
    "    adata, \n",
    "    n_genes=4, \n",
    "    key='marker_genes', \n",
    "    groupby='GCN_leiden_clusters'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = adata.obs['GCN_leiden_clusters'].unique()\n",
    "sq.pl.spatial_scatter(\n",
    "    adata,\n",
    "    library_id=\"spatial\",\n",
    "    color=\"GCN_leiden_clusters\",\n",
    "    shape=None,\n",
    "    title=\"Cellular Neighborhoods Identified by GCN Clustering\",\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "_, axes = plt.subplots(1, len(clusters) , figsize=(30, 5), sharey=True)\n",
    "for idx, cluster in enumerate(clusters):\n",
    "        \n",
    "    top_marker_gene_for_cluster_2 = adata.uns['marker_genes']['names'][cluster][0]\n",
    "\n",
    "    axes[idx].set_title(f\"Cluster {cluster} - Top Marker Gene: {top_marker_gene_for_cluster_2}\")\n",
    "    sq.pl.spatial_scatter(\n",
    "        adata,\n",
    "        library_id=\"spatial\",\n",
    "        color=top_marker_gene_for_cluster_2,\n",
    "        shape=None,\n",
    "        title=f\"Cluster: {cluster}\\n{top_marker_gene_for_cluster_2}\",\n",
    "        ax=axes[idx],\n",
    "        colorbar = False,\n",
    "    )\n",
    "    axes[idx].set_ylabel(\"\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "    axes[idx].set_aspect(\"equal\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "* The GNN successfully segmented the tissue into distinct, spatially coherent clusters that correspond to real, localized biological niches each defined by a unique top gene.\n",
    "\n",
    "* The embeddings are high-quality - well-separated in the latent space meaning the GCN learned distinct feature representations for each neighborhood.\n",
    "\n",
    "* The clusters correspond to biological effects based on the fact that the GCN Clusters appear in similar locations to the features in the Total Counts plot as well as regions of localized gene expression. Key Takeaways:\n",
    "\n",
    "  * **Cluster 0 and 8**: These clusters are defined by an adaptive immune response since CD74 is essential for antigen presentation and IGLC2 is an immunoglobulin. These pinpoint regions dense with B-cells and plasma cells possibly forming an organized structure to fight the tumor. \n",
    "\n",
    "  * **Cluster 1**: This cluster possibly represents an highly proliferative and aggressive cancer region given that MALAT1 is suspected to be associated with metastasis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "The primary findings include:\n",
    "* ...\n",
    "\n",
    "Next Steps:\n",
    "* Investigate the effect of different model architectures on the results. How do deeper networks affect the embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
